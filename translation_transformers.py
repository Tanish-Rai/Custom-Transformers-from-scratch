# -*- coding: utf-8 -*-
"""Translation Transformers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BD65K1BoBmPW1UsYI98eDef22gVqgBt1
"""

import pandas as pd
from sklearn.model_selection import train_test_split
import math
import torch
import torch.nn as nn
import torch.nn.functional as F

file_path = '/content/eng_-french.xlsx'
data = pd.read_excel(file_path)

# Define the column names based on the dataset
english_col = 'English words/sentences'
french_col = 'French words/sentences'

print("Dataset Head:")
print(data.head())
print("\nDataset Description:")
print(data.describe())
print("\nDataset Info:")
print(data.info())

train_data, test_data = train_test_split(data, test_size=0.3, random_state=42)

print(f"\nTraining Data Shape: {train_data.shape}")
print(f"Testing Data Shape: {test_data.shape}")

!pip install torch

import torch
import torch.nn as nn
import re
import unicodedata

def clean_text(text):
    # Normalize unicode characters
    text = unicodedata.normalize('NFD', text)
    text = text.encode('ascii', 'ignore').decode('utf-8')
    # Convert to lowercase
    text = text.lower()
    # Remove punctuation
    text = re.sub(r'[^\w\s]', '', text)
    return text

# Create a simple tokenizer
def tokenize(sentence):
    sentence = clean_text(sentence)
    return sentence.split()

def build_vocab(sentences):
    vocab = {}
    for sentence in sentences:
        cleaned_sentence = clean_text(sentence)
        for word in cleaned_sentence.split():
            if word not in vocab:
                vocab[word] = len(vocab) + 1  # Start indexing from 1
    return vocab

# Tokenize and encode sentences
def encode_sentence(sentence, vocab):
    tokens = tokenize(sentence)
    return [vocab.get(token, 0) for token in tokens]  # Use 0 for unknown tokens

# Create embedding layer
class EmbeddingLayer(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super(EmbeddingLayer, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)

    def forward(self, x):
        return self.embedding(x)

# Rebuild the vocabularies
english_vocab = build_vocab(train_data[english_col])
french_vocab = build_vocab(train_data[french_col])

# Define embedding dimensions
embedding_dim = 512

# Create embedding layers
english_embedding_layer = EmbeddingLayer(len(english_vocab) + 1, embedding_dim)
french_embedding_layer = EmbeddingLayer(len(french_vocab) + 1, embedding_dim)

# Example: Encode sentences and get embeddings
sample_english_sentence = train_data.iloc[0][english_col]
sample_french_sentence = train_data.iloc[0][french_col]

encoded_english = torch.tensor(encode_sentence(sample_english_sentence, english_vocab))
encoded_french = torch.tensor(encode_sentence(sample_french_sentence, french_vocab))

embedded_english = english_embedding_layer(encoded_english)
embedded_french = french_embedding_layer(encoded_french)

print("Embedded English:", embedded_english)
print("Embedded French:", embedded_french)

# Positional Encoding
embedding_dim = 512
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return x

# Create a positional encoding layer
pos_encoder = PositionalEncoding(embedding_dim)

# Apply positional encoding to the embedded sentences
pos_encoded_english = pos_encoder(embedded_english)
pos_encoded_french = pos_encoder(embedded_french)

print("Positional Encoded English:", pos_encoded_english)
print("Positional Encoded French:", pos_encoded_french)

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super(MultiHeadAttention, self).__init__()
        assert d_model % num_heads == 0
        self.d_k = d_model // num_heads
        self.num_heads = num_heads

        self.q_linear = nn.Linear(d_model, d_model)
        self.k_linear = nn.Linear(d_model, d_model)
        self.v_linear = nn.Linear(d_model, d_model)
        self.out = nn.Linear(d_model, d_model)

    def forward(self, q, k, v):
        bs = q.size(0)
        q = self.q_linear(q).view(bs, -1, self.num_heads, self.d_k).transpose(1, 2)
        k = self.k_linear(k).view(bs, -1, self.num_heads, self.d_k).transpose(1, 2)
        v = self.v_linear(v).view(bs, -1, self.num_heads, self.d_k).transpose(1, 2)

        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)
        scores = F.softmax(scores, dim=-1)
        output = torch.matmul(scores, v).transpose(1, 2).contiguous().view(bs, -1, self.num_heads * self.d_k)
        output = self.out(output)
        return output

class TransformerBlock(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super(TransformerBlock, self).__init__()
        self.attention = MultiHeadAttention(d_model, num_heads)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.ff = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Linear(d_ff, d_model)
        )
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)

    def forward(self, x):
        attn_out = self.attention(x, x, x)
        x = self.norm1(x + self.dropout1(attn_out))
        ff_out = self.ff(x)
        x = self.norm2(x + self.dropout2(ff_out))
        return x

# Encoder layer
class EncoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super(EncoderLayer, self).__init__()
        self.transformer_block = TransformerBlock(d_model, num_heads, d_ff, dropout)

    def forward(self, x):
        return self.transformer_block(x)

# Decoder layer
class DecoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super(DecoderLayer, self).__init__()
        self.attention = MultiHeadAttention(d_model, num_heads)
        self.norm1 = nn.LayerNorm(d_model)
        self.transformer_block = TransformerBlock(d_model, num_heads, d_ff, dropout)
        self.norm2 = nn.LayerNorm(d_model)

    def forward(self, x, enc_out):
        attn_out = self.attention(x, x, x)
        x = self.norm1(x + attn_out)
        out = self.transformer_block(x)
        return self.norm2(out + x)

# Encoder
class Encoder(nn.Module):
    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, max_len, dropout=0.1):
        super(Encoder, self).__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.positional_encoding = PositionalEncoding(d_model, max_len)
        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])

    def forward(self, x):
        x = self.embedding(x) * math.sqrt(self.embedding.embedding_dim)
        x = self.positional_encoding(x)
        for layer in self.layers:
            x = layer(x)
        return x

# Decoder
class Decoder(nn.Module):
    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, max_len, dropout=0.1):
        super(Decoder, self).__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.positional_encoding = PositionalEncoding(d_model, max_len)
        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])

    def forward(self, x, enc_out):
        x = self.embedding(x) * math.sqrt(self.embedding.embedding_dim)
        x = self.positional_encoding(x)
        for layer in self.layers:
            x = layer(x, enc_out)
        return x

  # Transformer model
class Transformer(nn.Module):
    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, max_len, dropout=0.1):
        super(Transformer, self).__init__()
        self.encoder = Encoder(src_vocab_size, d_model, num_layers, num_heads, d_ff, max_len, dropout)
        self.decoder = Decoder(tgt_vocab_size, d_model, num_layers, num_heads, d_ff, max_len, dropout)
        self.fc_out = nn.Linear(d_model, tgt_vocab_size)

    def forward(self, src, tgt):
        enc_out = self.encoder(src)
        dec_out = self.decoder(tgt, enc_out)
        out = self.fc_out(dec_out)
        return out

import torch.optim as optim
from torch.nn.utils.rnn import pad_sequence

def train(model, train_loader, val_loader, criterion, optimizer, num_epochs):
    for epoch in range(num_epochs):
        model.train()
        train_loss = 0
        for src, tgt in train_loader:
            optimizer.zero_grad()
            output = model(src, tgt)
            loss = criterion(output.view(-1, output.size(-1)), tgt.view(-1))
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
        train_loss /= len(train_loader)

        model.eval()
        val_loss = 0
        with torch.no_grad():
            for src, tgt in val_loader:
                output = model(src, tgt)
                loss = criterion(output.view(-1, output.size(-1)), tgt.view(-1))
                val_loss += loss.item()
        val_loss /= len(val_loader)

        print(f'Epoch: {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')

from torch.utils.data import DataLoader, Dataset

class TranslationDataset(Dataset):
    def __init__(self, src_sentences, tgt_sentences, src_vocab, tgt_vocab):
        self.src_sentences = src_sentences
        self.tgt_sentences = tgt_sentences
        self.src_vocab = src_vocab
        self.tgt_vocab = tgt_vocab

    def __len__(self):
        return len(self.src_sentences)

    def __getitem__(self, idx):
        src = torch.tensor(encode_sentence(self.src_sentences[idx], self.src_vocab))
        tgt = torch.tensor(encode_sentence(self.tgt_sentences[idx], self.tgt_vocab))
        return src, tgt

train_dataset = TranslationDataset(train_data[english_col].tolist(), train_data[french_col].tolist(), english_vocab, french_vocab)
val_dataset = TranslationDataset(test_data[english_col].tolist(), test_data[french_col].tolist(), english_vocab, french_vocab)

def collate_fn(batch):
    src_batch, tgt_batch = zip(*batch)
    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=0)
    tgt_batch = pad_sequence(tgt_batch, batch_first=True, padding_value=0)
    return src_batch, tgt_batch

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)

import torch.optim as optim
from torch.nn.utils.rnn import pad_sequence

# Use a very small subset of the dataset for quick test
subset_size = 100
train_subset = train_data[:subset_size]
val_subset = test_data[:subset_size]

# DataLoader for minimal batches
train_dataset = TranslationDataset(train_subset[english_col].tolist(), train_subset[french_col].tolist(), english_vocab, french_vocab)
val_dataset = TranslationDataset(val_subset[english_col].tolist(), val_subset[french_col].tolist(), english_vocab, french_vocab)

def collate_fn(batch):
    src_batch, tgt_batch = zip(*batch)
    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=0)
    tgt_batch = pad_sequence(tgt_batch, batch_first=True, padding_value=0)
    return src_batch, tgt_batch

train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)
val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)

# Define configurations and train models with minimal settings
configs = [
    {'num_heads': 1, 'num_layers': 1, 'd_model': 16},  # d_model must be divisible by num_heads
    {'num_heads': 8, 'num_layers': 1, 'd_model': 32},  # Updated d_model to be divisible by 8
    {'num_heads': 32, 'num_layers': 1, 'd_model': 64}, # Updated d_model to be divisible by 32
    {'num_heads': 8, 'num_layers': 2, 'd_model': 32},  # Updated d_model to be divisible by 8
]

# Training settings for quick test
d_ff = 32  # Minimal feed-forward dimension for faster testing
src_vocab_size = len(english_vocab) + 1
tgt_vocab_size = len(french_vocab) + 1
max_len = 100  # Shorter sequences for quick test
num_epochs = 5
learning_rate = 0.01

for config in configs:
    model = Transformer(
        src_vocab_size=src_vocab_size,
        tgt_vocab_size=tgt_vocab_size,
        d_model=config['d_model'],
        num_layers=config['num_layers'],
        num_heads=config['num_heads'],
        d_ff=d_ff,
        max_len=max_len
    )
    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Assuming padding index is 0
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    print(f"Training model with {config['num_heads']} heads and {config['num_layers']} layers")
    train(model, train_loader, val_loader, criterion, optimizer, num_epochs)

def evaluate(model, data_loader, criterion):
    model.eval()
    total_loss = 0

    with torch.no_grad():
        for src, tgt in data_loader:
            output = model(src, tgt)
            loss = criterion(output.view(-1, output.size(-1)), tgt.view(-1))
            total_loss += loss.item()

    average_loss = total_loss / len(data_loader)
    return average_loss

# Convert predictions and targets to sentences for more human-readable evaluation
def decode_sentence(encoded_sentence, vocab):
    reverse_vocab = {idx: word for word, idx in vocab.items()}
    return ' '.join([reverse_vocab[idx] for idx in encoded_sentence if idx != 0])

def display_sample_predictions(model, data_loader, src_vocab, tgt_vocab):
    model.eval()
    with torch.no_grad():
        for src, tgt in data_loader:
            output = model(src, tgt)
            predictions = output.argmax(dim=-1).cpu().numpy()
            targets = tgt.cpu().numpy()

            src_sentence = decode_sentence(src[0].cpu().numpy(), src_vocab)
            tgt_sentence = decode_sentence(targets[0], tgt_vocab)
            pred_sentence = decode_sentence(predictions[0], tgt_vocab)

            print(f"Source: {src_sentence}")
            print(f"Target: {tgt_sentence}")
            print(f"Prediction: {pred_sentence}")
            break  # Display one example

# Evaluate each model configuration
for config in configs:
    model = Transformer(
        src_vocab_size=src_vocab_size,
        tgt_vocab_size=tgt_vocab_size,
        d_model=config['d_model'],
        num_layers=config['num_layers'],
        num_heads=config['num_heads'],
        d_ff=d_ff,
        max_len=max_len
    )
    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Assuming padding index is 0
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    print(f"Training model with {config['num_heads']} heads and {config['num_layers']} layers")
    train(model, train_loader, val_loader, criterion, optimizer, num_epochs)

    test_loss = evaluate(model, val_loader, criterion)
    print(f"Test Loss: {test_loss:.4f}")
    display_sample_predictions(model, val_loader, english_vocab, french_vocab)